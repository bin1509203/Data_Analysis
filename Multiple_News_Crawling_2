""" 한겨레 신문 특정 키워드를 포함하는, 특정 날짜 이전 기사 내용 크롤러(정확도순 검색)
    python [모듈이름] [키워드] [가져올 페이지 숫자] [가져올 기사의 최근 날짜]
    [결과 파일명.txt]
    한페이지에 10개
"""
 
import sys
from bs4 import BeautifulSoup
import urllib.request
from urllib.parse import quote
 
# 1 페이지 = http://search.hani.co.kr/Search?command=query&keyword=%EC%82%AC%EB%93%9C
#           &sort=s&period=all&media=news
# 2 페이지 = http://search.hani.co.kr/Search?command=query&keyword=%EC%82%AC%EB%93%9C
#           &media=news&sort=s&period=all&datefrom=2000.01.01&dateto=2017.01.30&pageseq=1
# 3 페이지 = http://search.hani.co.kr/Search?command=query&keyword=%EC%82%AC%EB%93%9C
#           &media=news&sort=s&period=all&datefrom=2000.01.01&dateto=2017.01.30&pageseq=2
# 4 페이지 = http://search.hani.co.kr/Search?command=query&keyword=%EC%82%AC%EB%93%9C
#           &media=news&sort=s&period=all&datefrom=2000.01.01&dateto=2017.01.30&pageseq=3
# 규칙 : page_index = page - 1

TARGET_URL_BEFORE_KEWORD = 'http://search.hani.co.kr/Search?command=query&keyword='
TARGET_URL_BEFORE_UNTIL_DATE = '&media=news&sort=s&period=all&datefrom=2000.01.01&dateto='
TARGET_URL_REST = '&pageseq='
 
# 기사 검색 페이지에서 기사 제목에 링크된 기사 본문 주소 받아오기
def get_link_from_news_title(page_num, URL, output_file):
    for i in range(page_num): # page_index = page - 1 이고 i는 0 ~ page_num - 1 까지라 그대로 i  사용하면됨.
        URL_with_page_num = URL + str(i) # 페이지 index가 URL 맨 뒤에 위치하기 때문에 코드가 간결해졌다.
        # 즉, current_page_num = 1 + i*15 대신, current_page_num = i 인 꼴.
        # 페이지 index 가 URL 내부에 위치한 경우 다음과 같은 두줄의 코드가 필요하다.
        # position = URL.index('=')
        # URL_with_page_num = URL[: position+1] + str(current_page_num) + URL[position+1 :]
        source_code_from_URL = urllib.request.urlopen(URL_with_page_num)
        soup = BeautifulSoup(source_code_from_URL, 'lxml',
                             from_encoding='utf-8')

        # <dl>태그는 정의 리스트를 생성 / <dt>태그는 제목을 지정 / <dd>태그는 contents 
        # soup.select('A > B') => A 태그의 directly beneath 에 있는 B 태그를 선택해라!
        for item in soup.select('dt > a'):
            article_URL = item['href'] # 'href'로 연결된 URL주소를 'article_URL'에 저장
            get_text(article_URL, output_file)
 

# 기사 본문 내용 긁어오기 (위 함수 내부에서 기사 본문 주소 받아 사용되는 함수)
def get_text(URL, output_file):
    source_code_from_url = urllib.request.urlopen(URL)
    soup = BeautifulSoup(source_code_from_url, 'lxml', from_encoding='utf-8')

    # 본문 내용이 div tag 중 class="text" 인 div tag 내부에 담겨 있다.
    content_of_article = soup.select('div.text')

    for item in content_of_article:
        string_item = str(item.find_all(text=True)) # 'text=True'를 통해 해당 줄의 text 요소만 추출
        output_file.write(string_item) # 추출한 text를 파일에 write
 
def main() :
    keyword = '탄핵'
    page_num = int('2')
    until_date = '2017.01.30'
    output_file_name = 'M:\\Python_temp\\Multiple_News_Crawling_2.txt'
    target_URL = TARGET_URL_BEFORE_KEWORD + quote(keyword) \
                + TARGET_URL_BEFORE_UNTIL_DATE + until_date + TARGET_URL_REST
    output_file = open(output_file_name, 'w')
    get_link_from_news_title(page_num, target_URL, output_file)
    output_file.close()

if __name__ == '__main__' :
    main()

""" 
def main(argv):
    if len(sys.argv) != 5:
        print("python [모듈이름] [키워드] [가져올 페이지 숫자] "
              "[가져올 기사의 최근 날짜] [결과 파일명.txt]")
        return
    keyword = argv[1]
    page_num = int(argv[2])
    until_date = argv[3]
    output_file_name = argv[4]
    target_URL = TARGET_URL_BEFORE_KEWORD + quote(keyword) \
                 + TARGET_URL_BEFORE_UNTIL_DATE + until_date + TARGET_URL_REST

    output_file = open(output_file_name, 'w')
    get_link_from_news_title(page_num, target_URL, output_file)
    output_file.close()
 
 
if __name__ == '__main__':
    main(sys.argv)
"""


# 출처: http://yoonpunk.tistory.com/6 [윤빵꾸의 공부노트]
